{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVwc4RRXTJu+2lRA3HlVu3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prishi99/WomenLine-AI-/blob/main/Week_3_Language_Handler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzV1KzsVuiJD",
        "outputId": "8b332ed8-22dc-401b-e65a-63f734baa713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m553.0/981.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.7.14)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans, langdetect\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=5d63eecabbab96fa9d3881f15268199871704395dc354a85df2d7e0c1fa43fba\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=01f367bdacc0edfb06450c3e07b3caa953a6c5ad93c9de26f93d097bb11b536d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built googletrans langdetect\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, langdetect, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.27.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.97.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio 5.38.2 requires httpx<1.0,>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio-client 1.11.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.4.8 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 langdetect-1.0.9 rfc3986-1.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install langdetect googletrans==4.0.0-rc1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "\n",
        "# Load English dataset\n",
        "with open('legal_dataset.json', 'r', encoding='utf-8') as f:\n",
        "    english_dataset = json.load(f)\n",
        "\n",
        "# Load Translated Datasets (Optional)\n",
        "try:\n",
        "    with open('legal_qa_dataset_hi.json', 'r', encoding='utf-8') as f:\n",
        "        hindi_dataset = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    hindi_dataset = []\n",
        "\n",
        "try:\n",
        "    with open('legal_qa_dataset_ta.json', 'r', encoding='utf-8') as f:\n",
        "        tamil_dataset = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    tamil_dataset = []\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "def find_answer_from_dataset(dataset, question):\n",
        "    for entry in dataset:\n",
        "        if entry['question'].strip().lower() == question.strip().lower():\n",
        "            return entry['answer']\n",
        "    return None\n",
        "\n",
        "def handle_query(user_input):\n",
        "    # Detect language\n",
        "    detected_lang = detect(user_input)\n",
        "    print(f\"Detected language: {detected_lang}\")\n",
        "\n",
        "    # If English, search directly\n",
        "    if detected_lang == 'en':\n",
        "        return find_answer_from_dataset(english_dataset, user_input)\n",
        "\n",
        "    # Use translated dataset if available\n",
        "    if detected_lang == 'hi':\n",
        "        answer = find_answer_from_dataset(hindi_dataset, user_input)\n",
        "        if answer:\n",
        "            return answer\n",
        "    elif detected_lang == 'ta':\n",
        "        answer = find_answer_from_dataset(tamil_dataset, user_input)\n",
        "        if answer:\n",
        "            return answer\n",
        "\n",
        "    # Fallback: Translate to English, search, then translate answer back\n",
        "    translated_question = translator.translate(user_input, src=detected_lang, dest='en').text\n",
        "    print(f\"Translated question to English: {translated_question}\")\n",
        "\n",
        "    english_answer = find_answer_from_dataset(english_dataset, translated_question)\n",
        "\n",
        "    if english_answer:\n",
        "        translated_answer = translator.translate(english_answer, src='en', dest=detected_lang).text\n",
        "        return translated_answer\n",
        "\n",
        "    return \"Sorry, I couldn't find an answer to your question.\"\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        user_q = input(\"\\nAsk a legal question (type 'exit' to stop): \")\n",
        "        if user_q.lower() == 'exit':\n",
        "            break\n",
        "        print(\"Answer:\", handle_query(user_q))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8axF4yGVMTho",
        "outputId": "41c11f12-550f-45c2-ca20-1ef8fdf37ae0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ask a legal question (type 'exit' to stop): My husband beats me \n",
            "Detected language: en\n",
            "Answer: None\n",
            "\n",
            "Ask a legal question (type 'exit' to stop): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_dataset(dataset, target_lang, output_file):\n",
        "    translated = []\n",
        "    for entry in dataset:\n",
        "        translated.append({\n",
        "            \"intent\": entry[\"intent\"],\n",
        "            \"question\": translator.translate(entry[\"question\"], src='en', dest=target_lang).text,\n",
        "            \"answer\": translator.translate(entry[\"answer\"], src='en', dest=target_lang).text\n",
        "        })\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(translated, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Usage\n",
        "# translate_dataset(english_dataset, 'hi', 'legal_qa_dataset_hi.json')\n",
        "# translate_dataset(english_dataset, 'ta', 'legal_qa_dataset_ta.json')\n"
      ],
      "metadata": {
        "id": "VGoz3E0xMXZ3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "from googletrans import Translator\n",
        "\n",
        "# Initialize the Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "# Supported languages you want to translate to\n",
        "languages = {\n",
        "    'hi': 'Hindi',\n",
        "    'ta': 'Tamil',\n",
        "    'bn': 'Bengali',\n",
        "    'mr': 'Marathi'\n",
        "}\n",
        "\n",
        "# Load original English dataset\n",
        "with open('legal_dataset.json', 'r', encoding='utf-8') as f:\n",
        "    english_dataset = json.load(f)\n",
        "\n",
        "def translate_dataset(dataset, target_lang_code):\n",
        "    translated = []\n",
        "    for entry in dataset:\n",
        "        try:\n",
        "            translated_q = translator.translate(entry['question'], src='en', dest=target_lang_code).text\n",
        "            translated_a = translator.translate(entry['answer'], src='en', dest=target_lang_code).text\n",
        "\n",
        "            translated.append({\n",
        "                'intent': entry['intent'],\n",
        "                'question': translated_q,\n",
        "                'answer': translated_a\n",
        "            })\n",
        "\n",
        "            # Respect API limits\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating: {entry['question']}\\n{e}\\n\")\n",
        "            continue\n",
        "\n",
        "    return translated\n",
        "\n",
        "# Translate and save to separate JSON files\n",
        "for lang_code, lang_name in languages.items():\n",
        "    print(f\"\\nTranslating to {lang_name} ({lang_code})...\")\n",
        "    translated_dataset = translate_dataset(english_dataset, lang_code)\n",
        "\n",
        "    output_file = f'legal_qa_dataset_{lang_code}.json'\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(translated_dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"âœ… Saved translated dataset to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2RXt0FXMuLw",
        "outputId": "6d5eefb8-9768-48ef-fec3-820cd56e7f26"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translating to Hindi (hi)...\n",
            "Error translating: Does the cybercrime portal have a mobile-friendly or app version?\n",
            "The read operation timed out\n",
            "\n",
            "âœ… Saved translated dataset to legal_qa_dataset_hi.json\n",
            "\n",
            "Translating to Tamil (ta)...\n",
            "âœ… Saved translated dataset to legal_qa_dataset_ta.json\n",
            "\n",
            "Translating to Bengali (bn)...\n",
            "âœ… Saved translated dataset to legal_qa_dataset_bn.json\n",
            "\n",
            "Translating to Marathi (mr)...\n",
            "âœ… Saved translated dataset to legal_qa_dataset_mr.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "\n",
        "# Load translator\n",
        "translator = Translator()\n",
        "\n",
        "# Load all datasets\n",
        "DATASETS = {}\n",
        "LANGUAGES = ['en', 'hi', 'ta', 'bn', 'mr']\n",
        "\n",
        "# Dataset file paths\n",
        "DATASET_FILES = {\n",
        "    'en': 'legal_dataset.json',\n",
        "    'hi': 'legal_qa_dataset_hi.json',\n",
        "    'ta': 'legal_qa_dataset_ta.json',\n",
        "    'bn': 'legal_qa_dataset_bn.json',\n",
        "    'mr': 'legal_qa_dataset_mr.json'\n",
        "}\n",
        "\n",
        "# Load datasets into memory\n",
        "for lang_code, file_name in DATASET_FILES.items():\n",
        "    if os.path.exists(file_name):\n",
        "        with open(file_name, 'r', encoding='utf-8') as f:\n",
        "            DATASETS[lang_code] = json.load(f)\n",
        "    else:\n",
        "        DATASETS[lang_code] = []\n",
        "\n",
        "# Match question by fuzzy normalized string\n",
        "def find_answer(dataset, user_question):\n",
        "    question_lower = user_question.strip().lower()\n",
        "    for entry in dataset:\n",
        "        if entry['question'].strip().lower() == question_lower:\n",
        "            return entry['answer']\n",
        "    return None\n",
        "\n",
        "def handle_query(user_input):\n",
        "    # Detect user input language\n",
        "    try:\n",
        "        detected_lang = detect(user_input)\n",
        "    except:\n",
        "        return \"Sorry, couldn't detect your language.\"\n",
        "\n",
        "    print(f\"Detected language: {detected_lang}\")\n",
        "\n",
        "    # Use available dataset\n",
        "    if detected_lang in DATASETS:\n",
        "        answer = find_answer(DATASETS[detected_lang], user_input)\n",
        "        if answer:\n",
        "            return answer\n",
        "\n",
        "    # Fallback: Translate question to English and get answer\n",
        "    try:\n",
        "        translated_question = translator.translate(user_input, src=detected_lang, dest='en').text\n",
        "        english_answer = find_answer(DATASETS['en'], translated_question)\n",
        "        if not english_answer:\n",
        "            return \"Sorry, no answer found for your question.\"\n",
        "\n",
        "        # Translate back to original language\n",
        "        translated_answer = translator.translate(english_answer, src='en', dest=detected_lang).text\n",
        "        return translated_answer\n",
        "    except Exception as e:\n",
        "        print(\"Translation error:\", e)\n",
        "        return \"Sorry, something went wrong with translation.\"\n",
        "\n",
        "# Demo / Test loop\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸ” Multilingual Legal QA System (Type 'exit' to quit)\")\n",
        "    while True:\n",
        "        user_q = input(\"\\nAsk your question: \")\n",
        "        if user_q.lower() == 'exit':\n",
        "            break\n",
        "        response = handle_query(user_q)\n",
        "        print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPDzUr8gNWFN",
        "outputId": "df7df50a-d439-43bc-cf9a-2e2f111d4a86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Multilingual Legal QA System (Type 'exit' to quit)\n",
            "\n",
            "Ask your question: à¤­à¤¾à¤°à¤¤ à¤®à¥‡à¤‚ à¤˜à¤°à¥‡à¤²à¥‚ à¤¦à¥à¤°à¥à¤µà¥à¤¯à¤µà¤¹à¤¾à¤° à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
            "Detected language: hi\n",
            "Answer: à¤˜à¤°à¥‡à¤²à¥‚ à¤¦à¥à¤°à¥à¤µà¥à¤¯à¤µà¤¹à¤¾à¤° à¤®à¥‡à¤‚ à¤˜à¤°à¥‡à¤²à¥‚ à¤¸à¤‚à¤¬à¤‚à¤§à¥‹à¤‚ à¤•à¥‡ à¤­à¥€à¤¤à¤° à¤¶à¤¾à¤°à¥€à¤°à¤¿à¤•, à¤­à¤¾à¤µà¤¨à¤¾à¤¤à¥à¤®à¤•, à¤¯à¥Œà¤¨, à¤®à¥Œà¤–à¤¿à¤• à¤”à¤° à¤†à¤°à¥à¤¥à¤¿à¤• à¤¦à¥à¤°à¥à¤µà¥à¤¯à¤µà¤¹à¤¾à¤° à¤¶à¤¾à¤®à¤¿à¤² à¤¹à¥ˆà¤‚à¥¤[à¤¸à¥à¤°à¥‹à¤¤: à¤˜à¤°à¥‡à¤²à¥‚ à¤¹à¤¿à¤‚à¤¸à¤¾ à¤…à¤§à¤¿à¤¨à¤¿à¤¯à¤®, 2005 à¤¸à¥‡ à¤®à¤¹à¤¿à¤²à¤¾à¤“à¤‚ à¤•à¥€ à¤¸à¥à¤°à¤•à¥à¤·à¤¾]\n",
            "\n",
            "Ask your question: à¤­à¤¾à¤°à¤¤ à¤®à¥‡à¤‚ à¤˜à¤°à¥‡à¤²à¥‚ à¤¦à¥à¤°à¥à¤µà¥à¤¯à¤µà¤¹à¤¾à¤° à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
            "Detected language: hi\n",
            "Answer: à¤˜à¤°à¥‡à¤²à¥‚ à¤¦à¥à¤°à¥à¤µà¥à¤¯à¤µà¤¹à¤¾à¤° à¤®à¥‡à¤‚ à¤˜à¤°à¥‡à¤²à¥‚ à¤¸à¤‚à¤¬à¤‚à¤§à¥‹à¤‚ à¤•à¥‡ à¤­à¥€à¤¤à¤° à¤¶à¤¾à¤°à¥€à¤°à¤¿à¤•, à¤­à¤¾à¤µà¤¨à¤¾à¤¤à¥à¤®à¤•, à¤¯à¥Œà¤¨, à¤®à¥Œà¤–à¤¿à¤• à¤”à¤° à¤†à¤°à¥à¤¥à¤¿à¤• à¤¦à¥à¤°à¥à¤µà¥à¤¯à¤µà¤¹à¤¾à¤° à¤¶à¤¾à¤®à¤¿à¤² à¤¹à¥ˆà¤‚à¥¤[à¤¸à¥à¤°à¥‹à¤¤: à¤˜à¤°à¥‡à¤²à¥‚ à¤¹à¤¿à¤‚à¤¸à¤¾ à¤…à¤§à¤¿à¤¨à¤¿à¤¯à¤®, 2005 à¤¸à¥‡ à¤®à¤¹à¤¿à¤²à¤¾à¤“à¤‚ à¤•à¥€ à¤¸à¥à¤°à¤•à¥à¤·à¤¾]\n",
            "\n",
            "Ask your question: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mkbdm8EAgk9o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}